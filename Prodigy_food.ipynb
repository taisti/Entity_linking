{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1121c818",
   "metadata": {},
   "source": [
    "# Important imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e83b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import owlready2\n",
    "import prodigy\n",
    "import spacy \n",
    "import csv\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from prodigy.models.ner import EntityRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568dab82",
   "metadata": {},
   "source": [
    "### READ BRAT ANNOTATION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d56230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mendelai-brat-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2cd4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brat_parser import get_entities_relations_attributes_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list, relations_list = [], []\n",
    "\n",
    "for i in range(300):\n",
    "    entities, relations, _, _ = get_entities_relations_attributes_groups(r\".\\0-1-2\\{0}.ann\".format(i))\n",
    "    entities_list.append(entities)\n",
    "    relations_list.append(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entities_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relations_list[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8ed3f",
   "metadata": {},
   "source": [
    "### Candidate generation from ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31915d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_food_entities():\n",
    "    entities_loc = Path(\"food_product_entities.csv\")\n",
    "    \n",
    "    names = dict()\n",
    "    descriptions = dict()\n",
    "    \n",
    "    with entities_loc.open(\"r\", encoding=\"utf-8\") as csvfile: \n",
    "        csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "        \n",
    "        for row in csvreader:\n",
    "            qid = row[0]\n",
    "            name = row[1]\n",
    "            desc = row[2]\n",
    "            \n",
    "            names[qid] = name\n",
    "            descriptions[qid] = desc\n",
    "            \n",
    "    return names, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "name_dict, desc_dict = load_food_entities()\n",
    "kb = spacy.kb.KnowledgeBase(vocab=nlp.vocab, entity_vector_length=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f12a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for qid, desc in desc_dict.items():\n",
    "    desc_doc = nlp(desc)\n",
    "    desc_enc = desc_doc.vector\n",
    "    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)\n",
    "    \n",
    "for qid, name in name_dict.items():\n",
    "    name = re.sub(r'[^a-zA-Z0-9]', ' ', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "\n",
    "    for elem in name.split(' '):\n",
    "        if len(elem) > 0:\n",
    "            kb.add_alias(alias=elem, entities=[qid], probabilities=[1 / len(name.split(' '))])\n",
    "    kb.add_alias(alias=name, entities=[qid], probabilities=[1.0])\n",
    "\n",
    "qids = name_dict.keys()\n",
    "probs = [1 / len(qids) for _ in qids]\n",
    "kb.add_alias(alias=\"food\", entities=qids, probabilities=probs)\n",
    "\n",
    "'''print(f\"Entities in the KB: {kb.get_entity_strings()}\")\n",
    "print(f\"Aliases in the KB: {kb.get_alias_strings()}\")\n",
    "\n",
    "print(f\"Candidates for 'Roy Stanley Emerson': {[c.entity_ for c in kb.get_alias_candidates('Roy Stanley Emerson')]}\")\n",
    "print(f\"Candidates for 'Emerson': {[c.entity_ for c in kb.get_alias_candidates('Emerson')]}\")\n",
    "print(f\"Candidates for 'food': {[c.entity_ for c in kb.get_alias_candidates('food')]}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save KnowlegeBase \n",
    "output_dir = Path.cwd() / \"output_food\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir) \n",
    "\n",
    "kb.to_disk(output_dir / \"my_kb\")\n",
    "nlp.to_disk(output_dir / \"my_nlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8238cbc",
   "metadata": {},
   "source": [
    "### Manual Entity Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3311e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_option(stream, kb, id_dict):\n",
    "    for task in stream:\n",
    "        text = task[\"text\"]\n",
    "        \n",
    "        for span in task[\"spans\"]:\n",
    "            start_char = int(span[\"start\"])\n",
    "            end_char = int(span[\"end\"])\n",
    "            mention = text[start_char:end_char]\n",
    "            \n",
    "            candidates = []\n",
    "            for elem in mention.split(' '):\n",
    "                res = kb.get_alias_candidates(elem)\n",
    "                if res:\n",
    "                    candidates.append(res[0])\n",
    "                    \n",
    "            if candidates: \n",
    "                options = [{\"id\": c.entity_, \"html\": _print_url(c.entity_, id_dict)} for c in candidates]\n",
    "                options = sorted(options, key=lambda r: r[\"id\"])\n",
    "                options.append({\"id\": \"NIL_otherLink\", \"text\": \"Link not in options\"})\n",
    "                options.append({\"id\": \"NIL_ambiguous\", \"text\": \"Need more context\"})\n",
    "                task[\"options\"] = options\n",
    "                yield task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_url(entity_id, id_dict):\n",
    "    url_prefix = \"https://www.wikidata.org/wiki\"\n",
    "    name, descr = id_dict.get(entity_id)\n",
    "    option = \"<a href='\" + url_prefix + entity_id + \"'>\" + entity_id + \"</a>: \" + descr\n",
    "    return option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@prodigy.recipe(\n",
    "    \"entity_linker.manual\", \n",
    "    dataset=(\"The dataset to use\", \"positional\", None, str), \n",
    "    source=(\"The source data as a .txt file\", \"positional\", None, Path),\n",
    "    nlp_dir=(\"Path to the NLP model with a pretrained NER component\", \"positional\", None, Path),\n",
    "    kb_loc=(\"Path to the KB\", \"positional\", None, Path), \n",
    "    entity_loc=(\"Path to the file with additional information about he entities\", \"positional\", None, Path),\n",
    ")\n",
    "def entity_linker_manual(dataset, source, nlp_dir, kb_loc, entity_loc):\n",
    "    nlp = spacy.load(nlp_dir)\n",
    "    kb = spacy.kb.KnowledgeBase(vocab=nlp.vocab, entity_vector_length=1)\n",
    "    kb.from_disk(kb_loc)\n",
    "    model = EntityRecognizer(nlp)\n",
    "    \n",
    "    id_dict = {}\n",
    "    with entity_loc.open(\"r\", encoding=\"utf8\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "        for row in csvreader:\n",
    "            id_dict[row[0]] = (row[1], row[2])\n",
    "    \n",
    "    stream = prodigy.components.loaders.TXT(source)\n",
    "    stream = [prodigy.util.set_hashes(eg) for eg in stream]\n",
    "    stream = (eg for score, eg in model(stream))\n",
    "    \n",
    "    stream = _add_option(stream, kb, id_dict)\n",
    "    stream = prodigy.components.filters.filter_duplicates(stream, by_input=True, by_task=False)\n",
    "    \n",
    "    return {\n",
    "        \"dataset\": dataset, \n",
    "        \"stream\": stream,\n",
    "        \"view_id\": \"choice\",\n",
    "        \"config\": {\"choice_auto_accept\": True},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m prodigy entity_linker.manual food_sandbox .\\0-1-2\\ 3 ./output_food/my_nlp ./output_food/my_kb food_product_entities.csv -F sample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_number = 3\n",
    "file = Path(r\".\\0-1-2\\{0}.txt\".format(recipe_number)) # Path(\"emerson_input_text.txt\") # Path(r\".\\0-1-2\\0.txt\")\n",
    "model = EntityRecognizer(nlp)\n",
    "\n",
    "id_dict = {}\n",
    "entity_loc = Path(\"food_product_entities.csv\")\n",
    "kb = spacy.kb.KnowledgeBase(vocab=nlp.vocab, entity_vector_length=1)\n",
    "kb.from_disk(Path(\"./output_food/my_kb\"))\n",
    "\n",
    "with entity_loc.open(\"r\", encoding=\"utf8\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "    for row in csvreader:\n",
    "        id_dict[row[0]] = (row[1], row[2])\n",
    "\n",
    "stream = prodigy.components.loaders.TXT(file)\n",
    "stream = [prodigy.util.set_hashes(eg) for eg in stream]\n",
    "# print([elem for elem in stream])\n",
    "# print()\n",
    "# TODO: change it  \n",
    "# stream = (eg for score, eg in model(stream))\n",
    "# print([elem for elem in stream])\n",
    "for i, elem in enumerate(stream):\n",
    "    spans = []\n",
    "    for entity in entities_list[recipe_number].values():\n",
    "        if entity.text in elem['text']:\n",
    "            span = {'start': entity.span[0][0], 'end': entity.span[0][1], 'text': entity.text, 'label': entity.type}\n",
    "            spans.append(span)\n",
    "    stream[i]['spans'] = spans\n",
    "print(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for task in stream:\n",
    "    text = task[\"text\"]\n",
    "    print(text)\n",
    "        \n",
    "    for span in task[\"spans\"]:\n",
    "        start_char = int(span[\"start\"])\n",
    "        end_char = int(span[\"end\"])\n",
    "        mention = text[start_char:end_char]\n",
    "        \n",
    "        candidates = []\n",
    "        for elem in mention.split(' '):\n",
    "            res = kb.get_alias_candidates(elem)\n",
    "            if res:\n",
    "                candidates.append(res[0])\n",
    "        # print([elem.alias_ for elem in candidates])\n",
    "        \n",
    "        if candidates: \n",
    "            options = [{\"id\": c.entity_, \"html\": _print_url(c.entity_, id_dict)} for c in candidates]\n",
    "            options = sorted(options, key=lambda r: int(r[\"id\"][1:]))\n",
    "            options.append({\"id\": \"NIL_otherLink\", \"text\": \"Link not in options\"})\n",
    "            options.append({\"id\": \"NIL_ambiguous\", \"text\": \"Need more context\"})                \n",
    "            task[\"options\"] = options\n",
    "            yield task'''\n",
    "        \n",
    "stream = _add_option(stream, kb, id_dict)\n",
    "print([elem for elem in stream])\n",
    "stream = prodigy.components.filters.filter_duplicates(stream, by_input=True, by_task=False)\n",
    "print('final stream:', [elem for elem in stream])\n",
    "\n",
    "result = [id_dict[elem] for elem in id_dict.keys() if 'strawberry' in id_dict[elem][0] or 'strawberry' in id_dict[elem][1]]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642c3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
